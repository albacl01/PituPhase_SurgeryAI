{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5befc28d",
   "metadata": {},
   "source": [
    "# Downstream Evaluation using Reduced Training Subsets\n",
    " assess robustness by retraining/evaluating on reduced label subsets (50%, 25%, 10%, 5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d100951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of workers: 16\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from skimage import io, transform, util\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy.random as npr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f30a57",
   "metadata": {},
   "source": [
    "## 2. Downstream task (Fine-tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a652ab",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f80394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PituDataset(Dataset):\n",
    "    \"\"\"Pituitary Endoscopy dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, maxSize=0, unlabeled=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            maxSize (int, optional): Maximum size of the dataset (number of samples).\n",
    "            unlabeled (bool, optional): If True, ignore labels.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file, header=0, dtype={'id': str, 'label': int})\n",
    "        \n",
    "        if maxSize > 0:\n",
    "            newDatasetSize = maxSize  # maxSize samples (Parameter to select a specific number of images)\n",
    "            idx = np.random.RandomState(seed=42).permutation(range(len(self.dataset)))\n",
    "            reduced_dataset = self.dataset.iloc[idx[0:newDatasetSize]]\n",
    "            self.dataset = reduced_dataset.reset_index(drop=True)\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.unlabeled = unlabeled\n",
    "        self.classes = ['Desconocida', 'Preparacion colgajo', 'Etmoidectomia', 'Apertura selar', \n",
    "                        'Apertura dural', 'Reseccion tumoral', 'Cierre']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # Read the image\n",
    "        img_name = os.path.join(self.img_dir, self.dataset.id[idx] + '.png')\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        if self.unlabeled:\n",
    "            sample = {'image': image, 'label': np.int64(-1)}  # Use -1 to indicate unlabeled, keep datatype\n",
    "        else:\n",
    "            sample = {'image': image, 'label': self.dataset.label[idx].astype(dtype=np.long)}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f50e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "class Rescale(object):\n",
    "    \"\"\"Re-scale image to a predefined size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): The desired size. If it is a tuple, output is the output_size. \n",
    "        If it is an int, the smallest dimension will be the output_size\n",
    "            a we will keep fixed the original aspect ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'],sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'label' : label}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays into pytorch tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # Cambiamos los ejes\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        label=torch.tensor(label,dtype=torch.long)\n",
    "        \n",
    "        return {'image':image,\n",
    "                'label':label}\n",
    "    \n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize data by subtracting means and dividing by standard deviations.\n",
    "\n",
    "    Args:\n",
    "        mean_vec: Vector with means. \n",
    "        std_vec: Vector with standard deviations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean,std):\n",
    "      \n",
    "        assert len(mean)==len(std),'Length of mean and std vectors is not the same'\n",
    "        self.mean = np.array(mean)\n",
    "        self.std = np.array(std)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'],sample['label']\n",
    "        c, h, w = image.shape\n",
    "        assert c==len(self.mean), 'Length of mean and image is not the same' \n",
    "        dtype = image.dtype\n",
    "        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n",
    "        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n",
    "        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    \n",
    "        \n",
    "        return {'image': image, 'label' : label}\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crop the central area of the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tupla or int): Crop size. If int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        rem_h = h - new_h\n",
    "        rem_w = w - new_w\n",
    "        \n",
    "        if h>new_h:\n",
    "            top = int(rem_h/2)\n",
    "        else:\n",
    "            top=0\n",
    "            \n",
    "        if w>new_w: \n",
    "            left = int(rem_w/2)\n",
    "        else:\n",
    "            left = 0\n",
    "            \n",
    "        image = image[top: top + new_h,\n",
    "                     left: left + new_w]\n",
    "\n",
    "\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbb306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 213907\n",
      "Number of test examples: 56431\n"
     ]
    }
   ],
   "source": [
    "pixel_mean = [0.312, 0.120, 0.117]\n",
    "pixel_std = [0.280, 0.158, 0.160]\n",
    "\n",
    "img_transforms = transforms.Compose([CenterCrop((256, 320)),\n",
    "                                     Rescale((224,224)),\n",
    "                                     ToTensor(),\n",
    "                                     Normalize(mean=pixel_mean, std=pixel_std)])\n",
    "\n",
    "train_img_data = PituDataset(csv_file=\"/home/train_set.csv\",\n",
    "                                      root_dir='/home',\n",
    "                                      #maxSize=100000,\n",
    "                                      transform=img_transforms)\n",
    "\n",
    "val_img_data = PituDataset(csv_file=\"/home/val_set.csv\",\n",
    "                            root_dir='/home',\n",
    "                            transform=img_transforms)\n",
    "\n",
    "\n",
    "print(\"Number of training examples:\", len(train_img_data))\n",
    "print(\"Number of test examples:\", len(val_img_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81481b60",
   "metadata": {},
   "source": [
    "## Load the pre-trained model (SimCLR or BYOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a9cee",
   "metadata": {},
   "source": [
    "### SimCLR Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b899285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        \n",
    "        # Base model f(.): ResNet-50 \n",
    "        self.convnet = torchvision.models.resnet50()\n",
    "        in_features = self.convnet.fc.in_features  # 2048 for ResNet-50 (esto es h)\n",
    "\n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
    "        # this is the projection head: 2048 → 4 * hidden_dim → hidden_dim\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 4 * hidden_dim),  # Linear(2048, 4*hidden_dim) (entra h)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)  # Linear(4*hidden_dim, hidden_dim) (sale z)\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "        \n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        imgs = torch.cat(batch['image'], dim=0)\n",
    "        imgs = imgs.to(device=device, dtype=torch.float)  # shape: torch.Size([N_batch*2, 3, 224, 224])\n",
    "        \n",
    "        # Encode all images\n",
    "        feats = self.convnet(imgs) # esto es z, shape: torch.Size([N_batch*2, 128])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1) \n",
    "        \n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        \n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
    "        \n",
    "        # InfoNCE loss\n",
    "        cos_sim = cos_sim / self.hparams.temperature\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "        \n",
    "        ###--- Esto es para guardar los valores ---###\n",
    "        \n",
    "        # Logging loss\n",
    "        self.log(mode+'_loss', nll)\n",
    "        \n",
    "        # Get ranking position of positive example\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
    "                              cos_sim.masked_fill(pos_mask, -9e15)], \n",
    "                             dim=-1)\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
    "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
    "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
    "        \n",
    "        return nll\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bdd839",
   "metadata": {},
   "source": [
    "### BYOL Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb730f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    BYOL (Bootstrap Your Own Latent) implementation.\n",
    "\n",
    "    This class defines the architecture and training process for a self-supervised learning\n",
    "    model, allowing it to learn useful representations without using labeled data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, projection_size, lr, momentum, weight_decay, moving_average_decay):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim (int): The size of the hidden vector in the MLPs of the student and teacher projection heads.\n",
    "            projection_size (int): The size of the output vector from the projection head (dimension of the embedding space).\n",
    "            lr (float): Learning rate for the optimizer.\n",
    "            momentum (float): Momentum parameter for the SGD optimizer.\n",
    "            weight_decay (float): Weight decay for L2 regularization.\n",
    "            moving_average_decay (float): Decay factor for the exponential moving average used to update the teacher model. e.g. 0.99\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Base encoder f(.): ResNet-50\n",
    "        self.backbone = torchvision.models.resnet50()\n",
    "        in_features = self.backbone.fc.in_features  # 2048 for ResNet-50 (this is vector h)\n",
    "        self.backbone.fc = nn.Identity()  # Remove the final classification layer to get the feature vector\n",
    "        \n",
    "        # Projection head g(·) --> consists of Linear->BN->ReLU->Linear\n",
    "        self.student_projector = MLP(in_features, hidden_dim, projection_size) #(2048,4096,512)\n",
    "        \n",
    "        # Prediction head q(·)\n",
    "        self.student_predictor = MLP(projection_size, hidden_dim, projection_size)  #(512,4096,512) (output vector q)\n",
    "        \n",
    "        # Teacher model\n",
    "        self.teacher_projector = copy.deepcopy(self.student_projector)\n",
    "        \n",
    "        # EMA parameters\n",
    "        self.moving_average_decay = moving_average_decay\n",
    "        \n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        '''optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        '''\n",
    "        optimizer = optim.SGD(self.parameters(),\n",
    "                              lr=self.hparams.lr,\n",
    "                              weight_decay=self.hparams.weight_decay,\n",
    "                              momentum=self.hparams.momentum)\n",
    "        \n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_moving_average(self):\n",
    "        \"\"\"\n",
    "        Updates the weights of the teacher model as a moving average of the student model's weights.\n",
    "        \"\"\"\n",
    "        for student_params, teacher_params in zip(self.student_projector.parameters(), self.teacher_projector.parameters()):\n",
    "            teacher_params.data = teacher_params.data * self.moving_average_decay + (1. - self.moving_average_decay) * student_params.data\n",
    "          \n",
    "        \n",
    "    def initializes_target_network(self):\n",
    "        '''\n",
    "        Initializes the target (teacher) network with the same weights as the student model.\n",
    "        Ensures the teacher's parameters do not require gradient updates.\n",
    "        '''\n",
    "        \n",
    "        for student_params, teacher_params in zip(self.student_projector.parameters(), self.teacher_projector.parameters()):\n",
    "            teacher_params.data.copy_(student_params.data)  # initialize\n",
    "            teacher_params.requires_grad = False  # not update by gradient\n",
    "            \n",
    "    \n",
    "    def on_train_start(self):\n",
    "        # Initialize the teacher network at the start of training\n",
    "        self.initializes_target_network()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass through the student network and student projector\n",
    "        '''\n",
    "        features = self.backbone(x)\n",
    "        student_projection = self.student_projector(features)\n",
    "        student_prediction = self.student_predictor(student_projection)\n",
    "        return student_prediction\n",
    "    \n",
    "\n",
    "    def shared_step(self, img1, img2):\n",
    "    \n",
    "        # get student projections: backbone + MLP projection head\n",
    "        feats1 = self.backbone(img1) #this is h\n",
    "        feats2 = self.backbone(img2)\n",
    "        \n",
    "        student_proj1 = self.student_projector(feats1) #this is g\n",
    "        student_proj2 = self.student_projector(feats2)\n",
    "\n",
    "        # Apply the predictor MLP to the student's projections\n",
    "        student_pred1 = self.student_predictor(student_proj1) # this is q\n",
    "        student_pred2 = self.student_predictor(student_proj2)\n",
    "\n",
    "        # Get teacher projections (no gradient updates)\n",
    "        with torch.no_grad():\n",
    "            # teacher processes the images and makes projections: backbone + MLP\n",
    "            teacher_proj1 = self.teacher_projector(feats1) \n",
    "            teacher_proj2 = self.teacher_projector(feats2)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = L2_loss(student_pred1, teacher_proj2)\n",
    "        loss += L2_loss(student_pred2, teacher_proj1)        \n",
    "\n",
    "        return loss.mean() #loss = (loss1 + loss2).mean()\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img1, img2 = batch['image'][0], batch['image'][1]\n",
    "        img1 = img1.to(device=device, dtype=torch.float)\n",
    "        img2 = img2.to(device=device, dtype=torch.float)\n",
    "\n",
    "        loss = self.shared_step(img1, img2)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        # Update the teacher model\n",
    "        self.update_moving_average()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img1, img2 = batch['image'][0], batch['image'][1]\n",
    "        img1 = img1.to(device=device, dtype=torch.float)\n",
    "        img2 = img2.to(device=device, dtype=torch.float)\n",
    "\n",
    "        loss = self.shared_step(img1, img2)\n",
    "        self.log('val_loss', loss)\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2781d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimCLR(\n",
      "  (convnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the saved .pt model\n",
    "model_path = '/home/simclr_models/simclr_model.pt'\n",
    "\n",
    "# Initialize the model\n",
    "loaded_model = SimCLR(max_epochs=5, hidden_dim=64, lr=5e-4, temperature=0.07, weight_decay=1e-4)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "661b7f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discard everything except the encoder\n",
    "model=loaded_model.convnet\n",
    "\n",
    "# eliminate the last classification layer\n",
    "encoder = nn.Sequential(*list(model.children())[:-1])    \n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadce866",
   "metadata": {},
   "source": [
    "## Encode all images\n",
    "Next, we implement a small function to encode all images in our datasets. The output representations are then used as inputs to the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76e589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(encoder, dataset):\n",
    "    \"\"\"\n",
    "    Extracts features from the encoder for a given dataset and returns a TensorDataset.\n",
    "    \n",
    "    Args:\n",
    "        encoder (nn.Module): Pre-trained encoder model without the final classification layer.\n",
    "        dataset (Dataset): Dataset for which features need to be extracted.\n",
    "        \n",
    "    Returns:\n",
    "        TensorDataset: A dataset containing the extracted features and corresponding labels.\n",
    "    \"\"\"\n",
    "    # Set encoder to evaluation mode and move to the correct device\n",
    "    encoder.eval()\n",
    "    encoder = encoder.float()  # Ensure the encoder uses float precision\n",
    "    encoder.to(device)\n",
    "    \n",
    "    feats = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Prepare the data loader\n",
    "    data_loader = DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
    "\n",
    "    # Get the features from the pre-trained model\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        imgs = batch['image'].to(device, dtype=torch.float)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = encoder(imgs)\n",
    "            \n",
    "            feats.append(features.detach().cpu())\n",
    "            labels_list.append(labels.detach().cpu())\n",
    "    \n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "\n",
    "    return feats, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a00132",
   "metadata": {},
   "source": [
    "The original data loaders `train_loader, val_loader` are used to extract features from the images using a pre-trained encoder `encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34a2277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([213907, 2048, 1, 1]) torch.Size([213907])\n",
      "Testing data shape: torch.Size([56431, 2048, 1, 1]) torch.Size([56431])\n"
     ]
    }
   ],
   "source": [
    "# The function extracts features for each input image batch from the encoder\n",
    "x_train, y_train = prepare_data_features(encoder, train_img_data)\n",
    "x_test, y_test = prepare_data_features(encoder, val_img_data)\n",
    "\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711c7892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([213907, 2048]) torch.Size([213907])\n",
      "Testing data shape: torch.Size([56431, 2048]) torch.Size([56431])\n"
     ]
    }
   ],
   "source": [
    "# checks if the feature tensor has more than two dimensions: [N, feature_dim, height, width]\n",
    "if len(x_test.shape) > 2:\n",
    "    x_train = torch.mean(x_train, dim=[2, 3]) # reduce the shape of the features to [N, feature_dim]\n",
    "    x_test = torch.mean(x_test, dim=[2, 3])\n",
    "\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33020bb1",
   "metadata": {},
   "source": [
    "After feature extraction, the features `x_train, x_test` are standardized for better training of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bb9629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the extracted features: mean of 0 and a standard deviation of 1\n",
    "scaler = preprocessing.StandardScaler()  \n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train).astype(np.float32) # convert to float32 \n",
    "x_test = scaler.transform(x_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34250f9b",
   "metadata": {},
   "source": [
    "New data loaders are created to work with the scaled features instead of raw image data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ecddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    '''- Input: Takes feature vectors (X_train and X_test) and corresponding labels (y_train and y_test).\n",
    "        \n",
    "       - Purpose: Converts the feature arrays and their labels into TensorDataset objects. \n",
    "       This allows the features and labels to be combined as tensors, which is the format that PyTorch expects.'''\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    val_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c910f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_data_loaders_from_arrays(torch.from_numpy(x_train), y_train, torch.from_numpy(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137441a0",
   "metadata": {},
   "source": [
    "These new loaders `train_loader, val_loader` will be used to train a simple classifier (e.g., logistic regression) on the extracted, standardized features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862b6ef",
   "metadata": {},
   "source": [
    "## Training con subsets\n",
    "\n",
    "We will perform experiments with even smaller datasets. \n",
    "Specifically, we train a Logistic Regression model for datasets with 5%, 10%, 25% and 50% of images in our training set. This gives us an intuition on how well the representations learned by contrastive learning can be transfered to a image recognition task like this classification. \n",
    "\n",
    "First, let's define a function to create the intended sub-datasets from the full training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f03a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smaller_dataset(dataset, quota_dict, surgery_ids):\n",
    "    \"\"\"\n",
    "    Create a smaller dataset from a TensorDataset DataLoader.\n",
    "\n",
    "    Args:\n",
    "        original_loader (DataLoader): The original DataLoader.\n",
    "        quota_dict (dict): Dict specifying number of samples per class.\n",
    "        surgery_ids (list): List of surgery IDs for each sample in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Subset: A subset of the original dataset.\n",
    "    \"\"\"\n",
    "    #dataset = original_loader.dataset\n",
    "\n",
    "    labels = dataset.tensors[1]\n",
    "    unique_labels = torch.unique(labels)\n",
    "    indices = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label = label.item()\n",
    "        num_imgs_per_label = quota_dict[label]\n",
    "        label_indices = torch.where(labels == label)[0]\n",
    "        surgery_ids_for_label = [surgery_ids[i.item()] for i in label_indices]\n",
    "\n",
    "        surgery_indices_dict = {surg: [] for surg in set(surgery_ids_for_label)}\n",
    "        for idx, surg_id in zip(label_indices, surgery_ids_for_label):\n",
    "            surgery_indices_dict[surg_id].append(idx.item())\n",
    "\n",
    "        num_surgeries = len(surgery_indices_dict)\n",
    "        samples_per_surgery = num_imgs_per_label // num_surgeries\n",
    "        remaining_samples = num_imgs_per_label % num_surgeries\n",
    "\n",
    "        sampled_indices = []\n",
    "        leftover = []\n",
    "\n",
    "        for surg_id, surg_indices in surgery_indices_dict.items():\n",
    "            np.random.shuffle(surg_indices)\n",
    "            if len(surg_indices) >= samples_per_surgery:\n",
    "                sampled = surg_indices[:samples_per_surgery]\n",
    "                leftover.extend(surg_indices[samples_per_surgery:])\n",
    "            else:\n",
    "                sampled = surg_indices\n",
    "                remaining_samples += samples_per_surgery - len(sampled)\n",
    "            sampled_indices.extend(sampled)\n",
    "\n",
    "        if remaining_samples > 0 and leftover:\n",
    "            np.random.shuffle(leftover)\n",
    "            sampled_indices.extend(leftover[:remaining_samples])\n",
    "\n",
    "        indices.extend(sampled_indices)\n",
    "\n",
    "    return Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b021f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the subsets.\n",
    "# Since the number of frames per label is not consistent, we need to sample a percentage of images from each class.\n",
    "# The function 'calculate_distribution' returns the number of images per class based on the target subset size.\n",
    "\n",
    "def calculate_distribution(image_counts_per_label, target_size):\n",
    "    \"\"\"\n",
    "    Calculate how many images to sample from each class so that the\n",
    "    overall subset matches the target size, while preserving the class\n",
    "    distribution as closely as possible.\n",
    "\n",
    "    Parameters:\n",
    "        image_counts_per_label (dict): Mapping from label to total images available.\n",
    "        target_size (int): Desired total number of images in the subset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from label to number of images to sample for that class.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_images = sum(image_counts_per_label.values())\n",
    "    # Initial allocation proportional to class frequencies\n",
    "    quotas = {\n",
    "        label: int((count / total_images) * target_size)\n",
    "        for label, count in image_counts_per_label.items()\n",
    "    }\n",
    "    current_total = sum(quotas.values())\n",
    "\n",
    "    # Adjust quotas if rounding causes mismatch with target_size\n",
    "    if current_total < target_size:\n",
    "        # Distribute the remaining slots one by one\n",
    "        remainder = target_size - current_total\n",
    "        labels = list(image_counts_per_label.keys())\n",
    "        for i in range(remainder):\n",
    "            quotas[labels[i % len(labels)]] += 1\n",
    "    elif current_total > target_size:\n",
    "        # Remove the excess slots one by one\n",
    "        excess = current_total - target_size\n",
    "        labels = list(image_counts_per_label.keys())\n",
    "        for i in range(excess):\n",
    "            quotas[labels[i % len(labels)]] -= 1\n",
    "\n",
    "    return quotas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df47a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now obtain the train ids:\n",
    "df_train = pd.read_csv('/home/train_set.csv',sep=',', header=0)\n",
    "\n",
    "def get_patient_id(id_str):\n",
    "    return id_str.split('_')[0]\n",
    "\n",
    "ids = df_train['id']\n",
    "surgery_ids_train = ids.apply(get_patient_id).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7ea37",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Now we can apply the extracted characteristics on a supervised downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e86e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4834c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eval_metrics(predicted_labels, labels):\n",
    "    #Accuracy\n",
    "    accuracy = np.mean(predicted_labels == labels)\n",
    "    \n",
    "    # Recall y precision por clase\n",
    "    class_precision = []\n",
    "    class_recall = []\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    \n",
    "    for label in unique_labels: #Para cada clase\n",
    "        VP = np.sum((predicted_labels == label) & (labels == label)) #Number of correct detections \n",
    "        FP = np.sum((predicted_labels == label) & (labels != label)) #Number of incorrect detections\n",
    "        FN = np.sum((predicted_labels != label) & (labels == label)) \n",
    "\n",
    "        Precision = VP/(VP+FP)\n",
    "        Recall = VP/(VP+FN)\n",
    "\n",
    "        class_precision.append(Precision)\n",
    "        class_recall.append(Recall)\n",
    "    \n",
    "    precision = np.mean(class_precision)\n",
    "    recall = np.mean(class_recall)\n",
    "    \n",
    "    f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "        \n",
    "    return accuracy, class_precision, class_recall, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4f204d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionario\n",
    "original_dataset = train_loader.dataset\n",
    "\n",
    "labels = original_dataset.tensors[1]\n",
    "label_counts = Counter(labels.tolist())\n",
    "\n",
    "# Define los subgrupos que quieres\n",
    "subsets = [10700, 21400, 53500, 107000, 213907]  # Nº de imágenes totales en el conjunto\n",
    "\n",
    "target_sizes_and_quotas = {subset: calcular_distribucion(label_counts, subset) for subset in subsets}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5dae45",
   "metadata": {},
   "source": [
    "Finally, we can write a training function as usual. We evaluate the model on the validation set every 10 epochs to allow early stopping, but the low frequency of the validation ensures that we do not overfit too much on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e8e6571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with subset size: 10700\n",
      "Epoch 0:\n",
      "  - Train Loss: 1.5286\n",
      "  - Accuracy: 55.56%\n",
      "  - Precision: 44.82%\n",
      "  - Recall: 46.16%\n",
      "  - F1-Score: 45.48%\n",
      "\n",
      "Epoch 10:\n",
      "  - Train Loss: 1.1411\n",
      "  - Accuracy: 57.94%\n",
      "  - Precision: 49.22%\n",
      "  - Recall: 48.93%\n",
      "  - F1-Score: 49.07%\n",
      "\n",
      "Epoch 20:\n",
      "  - Train Loss: 1.0409\n",
      "  - Accuracy: 58.40%\n",
      "  - Precision: 49.33%\n",
      "  - Recall: 48.49%\n",
      "  - F1-Score: 48.91%\n",
      "\n",
      "Epoch 30:\n",
      "  - Train Loss: 0.9815\n",
      "  - Accuracy: 57.89%\n",
      "  - Precision: 49.62%\n",
      "  - Recall: 47.60%\n",
      "  - F1-Score: 48.59%\n",
      "\n",
      "Epoch 40:\n",
      "  - Train Loss: 0.9371\n",
      "  - Accuracy: 58.10%\n",
      "  - Precision: 49.75%\n",
      "  - Recall: 47.25%\n",
      "  - F1-Score: 48.47%\n",
      "\n",
      "Epoch 50:\n",
      "  - Train Loss: 0.9036\n",
      "  - Accuracy: 57.93%\n",
      "  - Precision: 49.44%\n",
      "  - Recall: 47.59%\n",
      "  - F1-Score: 48.49%\n",
      "\n",
      "Epoch 60:\n",
      "  - Train Loss: 0.8756\n",
      "  - Accuracy: 57.94%\n",
      "  - Precision: 49.15%\n",
      "  - Recall: 46.78%\n",
      "  - F1-Score: 47.94%\n",
      "\n",
      "Epoch 70:\n",
      "  - Train Loss: 0.8563\n",
      "  - Accuracy: 58.04%\n",
      "  - Precision: 48.75%\n",
      "  - Recall: 46.88%\n",
      "  - F1-Score: 47.79%\n",
      "\n",
      "Epoch 80:\n",
      "  - Train Loss: 0.8309\n",
      "  - Accuracy: 57.74%\n",
      "  - Precision: 49.08%\n",
      "  - Recall: 46.80%\n",
      "  - F1-Score: 47.91%\n",
      "\n",
      "Epoch 90:\n",
      "  - Train Loss: 0.8146\n",
      "  - Accuracy: 58.24%\n",
      "  - Precision: 49.41%\n",
      "  - Recall: 46.51%\n",
      "  - F1-Score: 47.92%\n",
      "\n",
      "\n",
      "Training with subset size: 21400\n",
      "Epoch 0:\n",
      "  - Train Loss: 1.4476\n",
      "  - Accuracy: 57.70%\n",
      "  - Precision: 48.27%\n",
      "  - Recall: 46.27%\n",
      "  - F1-Score: 47.25%\n",
      "\n",
      "Epoch 10:\n",
      "  - Train Loss: 1.0621\n",
      "  - Accuracy: 58.03%\n",
      "  - Precision: 49.92%\n",
      "  - Recall: 46.09%\n",
      "  - F1-Score: 47.93%\n",
      "\n",
      "Epoch 20:\n",
      "  - Train Loss: 0.9670\n",
      "  - Accuracy: 58.02%\n",
      "  - Precision: 48.92%\n",
      "  - Recall: 46.62%\n",
      "  - F1-Score: 47.74%\n",
      "\n",
      "Epoch 30:\n",
      "  - Train Loss: 0.9113\n",
      "  - Accuracy: 58.19%\n",
      "  - Precision: 49.54%\n",
      "  - Recall: 46.57%\n",
      "  - F1-Score: 48.01%\n",
      "\n",
      "Epoch 40:\n",
      "  - Train Loss: 0.8726\n",
      "  - Accuracy: 58.10%\n",
      "  - Precision: 49.41%\n",
      "  - Recall: 47.17%\n",
      "  - F1-Score: 48.26%\n",
      "\n",
      "Epoch 50:\n",
      "  - Train Loss: 0.8438\n",
      "  - Accuracy: 56.98%\n",
      "  - Precision: 48.30%\n",
      "  - Recall: 46.11%\n",
      "  - F1-Score: 47.18%\n",
      "\n",
      "Epoch 60:\n",
      "  - Train Loss: 0.8224\n",
      "  - Accuracy: 57.93%\n",
      "  - Precision: 49.64%\n",
      "  - Recall: 46.46%\n",
      "  - F1-Score: 48.00%\n",
      "\n",
      "Epoch 70:\n",
      "  - Train Loss: 0.8053\n",
      "  - Accuracy: 58.20%\n",
      "  - Precision: 49.74%\n",
      "  - Recall: 46.15%\n",
      "  - F1-Score: 47.88%\n",
      "\n",
      "Epoch 80:\n",
      "  - Train Loss: 0.7900\n",
      "  - Accuracy: 57.70%\n",
      "  - Precision: 49.36%\n",
      "  - Recall: 46.29%\n",
      "  - F1-Score: 47.78%\n",
      "\n",
      "Epoch 90:\n",
      "  - Train Loss: 0.7777\n",
      "  - Accuracy: 58.20%\n",
      "  - Precision: 49.83%\n",
      "  - Recall: 46.93%\n",
      "  - F1-Score: 48.34%\n",
      "\n",
      "\n",
      "Training with subset size: 53500\n",
      "Epoch 0:\n",
      "  - Train Loss: 1.3675\n",
      "  - Accuracy: 58.96%\n",
      "  - Precision: 50.86%\n",
      "  - Recall: 49.15%\n",
      "  - F1-Score: 49.99%\n",
      "\n",
      "Epoch 10:\n",
      "  - Train Loss: 0.9511\n",
      "  - Accuracy: 58.71%\n",
      "  - Precision: 50.71%\n",
      "  - Recall: 47.60%\n",
      "  - F1-Score: 49.10%\n",
      "\n",
      "Epoch 20:\n",
      "  - Train Loss: 0.8692\n",
      "  - Accuracy: 58.88%\n",
      "  - Precision: 50.27%\n",
      "  - Recall: 46.55%\n",
      "  - F1-Score: 48.34%\n",
      "\n",
      "Epoch 30:\n",
      "  - Train Loss: 0.8286\n",
      "  - Accuracy: 58.93%\n",
      "  - Precision: 50.00%\n",
      "  - Recall: 46.94%\n",
      "  - F1-Score: 48.42%\n",
      "\n",
      "Epoch 40:\n",
      "  - Train Loss: 0.8015\n",
      "  - Accuracy: 58.01%\n",
      "  - Precision: 49.30%\n",
      "  - Recall: 46.00%\n",
      "  - F1-Score: 47.59%\n",
      "\n",
      "Epoch 50:\n",
      "  - Train Loss: 0.7802\n",
      "  - Accuracy: 58.57%\n",
      "  - Precision: 50.06%\n",
      "  - Recall: 46.52%\n",
      "  - F1-Score: 48.22%\n",
      "\n",
      "Epoch 60:\n",
      "  - Train Loss: 0.7637\n",
      "  - Accuracy: 58.66%\n",
      "  - Precision: 49.91%\n",
      "  - Recall: 46.40%\n",
      "  - F1-Score: 48.09%\n",
      "\n",
      "Epoch 70:\n",
      "  - Train Loss: 0.7496\n",
      "  - Accuracy: 58.16%\n",
      "  - Precision: 49.35%\n",
      "  - Recall: 46.35%\n",
      "  - F1-Score: 47.80%\n",
      "\n",
      "Epoch 80:\n",
      "  - Train Loss: 0.7381\n",
      "  - Accuracy: 58.58%\n",
      "  - Precision: 49.18%\n",
      "  - Recall: 46.02%\n",
      "  - F1-Score: 47.54%\n",
      "\n",
      "Epoch 90:\n",
      "  - Train Loss: 0.7281\n",
      "  - Accuracy: 58.50%\n",
      "  - Precision: 49.93%\n",
      "  - Recall: 46.16%\n",
      "  - F1-Score: 47.97%\n",
      "\n",
      "\n",
      "Training with subset size: 107000\n",
      "Epoch 0:\n",
      "  - Train Loss: 1.2806\n",
      "  - Accuracy: 58.46%\n",
      "  - Precision: 49.05%\n",
      "  - Recall: 48.02%\n",
      "  - F1-Score: 48.53%\n",
      "\n",
      "Epoch 10:\n",
      "  - Train Loss: 0.8741\n",
      "  - Accuracy: 58.75%\n",
      "  - Precision: 50.29%\n",
      "  - Recall: 46.63%\n",
      "  - F1-Score: 48.39%\n",
      "\n",
      "Epoch 20:\n",
      "  - Train Loss: 0.8094\n",
      "  - Accuracy: 58.56%\n",
      "  - Precision: 50.63%\n",
      "  - Recall: 45.97%\n",
      "  - F1-Score: 48.19%\n",
      "\n",
      "Epoch 30:\n",
      "  - Train Loss: 0.7747\n",
      "  - Accuracy: 58.17%\n",
      "  - Precision: 49.74%\n",
      "  - Recall: 46.54%\n",
      "  - F1-Score: 48.09%\n",
      "\n",
      "Epoch 40:\n",
      "  - Train Loss: 0.7511\n",
      "  - Accuracy: 58.52%\n",
      "  - Precision: 50.08%\n",
      "  - Recall: 46.69%\n",
      "  - F1-Score: 48.33%\n",
      "\n",
      "Epoch 50:\n",
      "  - Train Loss: 0.7334\n",
      "  - Accuracy: 58.07%\n",
      "  - Precision: 49.24%\n",
      "  - Recall: 45.77%\n",
      "  - F1-Score: 47.44%\n",
      "\n",
      "Epoch 60:\n",
      "  - Train Loss: 0.7192\n",
      "  - Accuracy: 57.76%\n",
      "  - Precision: 50.00%\n",
      "  - Recall: 46.24%\n",
      "  - F1-Score: 48.05%\n",
      "\n",
      "Epoch 70:\n",
      "  - Train Loss: 0.7072\n",
      "  - Accuracy: 57.72%\n",
      "  - Precision: 48.83%\n",
      "  - Recall: 45.64%\n",
      "  - F1-Score: 47.18%\n",
      "\n",
      "Epoch 80:\n",
      "  - Train Loss: 0.6980\n",
      "  - Accuracy: 58.10%\n",
      "  - Precision: 49.41%\n",
      "  - Recall: 46.79%\n",
      "  - F1-Score: 48.06%\n",
      "\n",
      "Epoch 90:\n",
      "  - Train Loss: 0.6895\n",
      "  - Accuracy: 58.27%\n",
      "  - Precision: 49.26%\n",
      "  - Recall: 46.79%\n",
      "  - F1-Score: 47.99%\n",
      "\n",
      "\n",
      "Training with subset size: 213907\n",
      "Epoch 0:\n",
      "  - Train Loss: 1.2068\n",
      "  - Accuracy: 58.17%\n",
      "  - Precision: 50.55%\n",
      "  - Recall: 47.31%\n",
      "  - F1-Score: 48.88%\n",
      "\n",
      "Epoch 10:\n",
      "  - Train Loss: 0.8119\n",
      "  - Accuracy: 58.16%\n",
      "  - Precision: 49.81%\n",
      "  - Recall: 46.00%\n",
      "  - F1-Score: 47.83%\n",
      "\n",
      "Epoch 20:\n",
      "  - Train Loss: 0.7573\n",
      "  - Accuracy: 58.30%\n",
      "  - Precision: 49.64%\n",
      "  - Recall: 46.72%\n",
      "  - F1-Score: 48.13%\n",
      "\n",
      "Epoch 30:\n",
      "  - Train Loss: 0.7268\n",
      "  - Accuracy: 58.27%\n",
      "  - Precision: 49.07%\n",
      "  - Recall: 46.00%\n",
      "  - F1-Score: 47.48%\n",
      "\n",
      "Epoch 40:\n",
      "  - Train Loss: 0.7069\n",
      "  - Accuracy: 58.52%\n",
      "  - Precision: 49.54%\n",
      "  - Recall: 47.17%\n",
      "  - F1-Score: 48.33%\n",
      "\n",
      "Epoch 50:\n",
      "  - Train Loss: 0.6920\n",
      "  - Accuracy: 58.38%\n",
      "  - Precision: 49.74%\n",
      "  - Recall: 46.45%\n",
      "  - F1-Score: 48.04%\n",
      "\n",
      "Epoch 60:\n",
      "  - Train Loss: 0.6809\n",
      "  - Accuracy: 58.10%\n",
      "  - Precision: 48.98%\n",
      "  - Recall: 46.23%\n",
      "  - F1-Score: 47.57%\n",
      "\n",
      "Epoch 70:\n",
      "  - Train Loss: 0.6722\n",
      "  - Accuracy: 57.32%\n",
      "  - Precision: 48.56%\n",
      "  - Recall: 45.85%\n",
      "  - F1-Score: 47.16%\n",
      "\n",
      "Epoch 80:\n",
      "  - Train Loss: 0.6648\n",
      "  - Accuracy: 57.97%\n",
      "  - Precision: 48.99%\n",
      "  - Recall: 46.33%\n",
      "  - F1-Score: 47.62%\n",
      "\n",
      "Epoch 90:\n",
      "  - Train Loss: 0.6586\n",
      "  - Accuracy: 58.56%\n",
      "  - Precision: 49.86%\n",
      "  - Recall: 47.04%\n",
      "  - F1-Score: 48.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store metrics for each subset\n",
    "results = {\n",
    "    \"subset_size\": [],\n",
    "    \"best_epoch\": [],\n",
    "    \"best_accuracy\": [],\n",
    "    \"best_precision\": [],\n",
    "    \"best_recall\": [],\n",
    "    \"best_f1_score\": [],\n",
    "    \"all_preds\": [],\n",
    "    \"all_labels\": [],\n",
    "    \"best_model\": [],\n",
    "}\n",
    "n=100\n",
    "\n",
    "# Weighted cross entropy     \n",
    "num_etiquetas = [9378, 10254, 37582, 45600, 20624, 50000, 46586]    \n",
    "\n",
    "weights = []\n",
    "for num in num_etiquetas:\n",
    "    weight_for_class_i = sum(num_etiquetas) / num\n",
    "    weights.append(weight_for_class_i)\n",
    "    \n",
    "    \n",
    "for subset_size, quota_dict in target_sizes_and_quotas.items():\n",
    "    print(f\"\\nTraining with subset size: {subset_size}\")\n",
    "    \n",
    "    # 1. Create the smaller dataset for this subset size\n",
    "    train_subset = get_smaller_dataset(original_dataset, quota_dict, surgery_ids_train)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # 2. Train\n",
    "    output_feature_dim = loaded_model.convnet.fc[0].in_features # 2048\n",
    "    \n",
    "    # Initialize the logistic regression model and optimizer\n",
    "    logreg = LogisticRegression(output_feature_dim, 7)\n",
    "    logreg = logreg.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weights=weights)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.AdamW(logreg.parameters(), lr=0.0001, weight_decay = 0.01)\n",
    "\n",
    "    # Define the loss function\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    # Variables to store metrics for plotting\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "    val_f1_scores = []\n",
    "    \n",
    "    # Variables for best epoch\n",
    "    eval_every_n_epochs = 10\n",
    "    best_acc = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Start training\n",
    "    for epoch in range(n):\n",
    "        # Training loop\n",
    "        logreg.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = logreg(imgs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Evaluation loop every 10 epochs\n",
    "        if epoch % eval_every_n_epochs == 0:\n",
    "            logreg.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in val_loader:\n",
    "                    imgs = imgs.to(device).float()\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = logreg(imgs)\n",
    "                    predictions = torch.argmax(outputs, dim=1)\n",
    "                    \n",
    "                    # Store predictions and labels for metric calculation\n",
    "                    all_preds.extend(predictions.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Convert lists to numpy arrays for metric calculations\n",
    "            all_preds = np.array(all_preds)\n",
    "            all_labels = np.array(all_labels)\n",
    "            \n",
    "            # Compute metrics\n",
    "            accuracy, class_precision, class_recall, precision, recall, f1_score = compute_eval_metrics(all_preds, all_labels)\n",
    "            \n",
    "            # Store evaluation metrics\n",
    "            val_accuracies.append(accuracy)\n",
    "            val_precisions.append(precision)\n",
    "            val_recalls.append(recall)\n",
    "            val_f1_scores.append(f1_score)\n",
    "            \n",
    "            # Deep copy the best model if it has the best F1-score\n",
    "            if f1_score > best_acc:\n",
    "                best_acc = f1_score\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = copy.deepcopy(logreg.state_dict())\n",
    "                \n",
    "            # Print the results for the current epoch\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"  - Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "            print(f\"  - Precision: {precision * 100:.2f}%\")\n",
    "            print(f\"  - Recall: {recall * 100:.2f}%\")\n",
    "            print(f\"  - F1-Score: {f1_score * 100:.2f}%\\n\")\n",
    "\n",
    "    # Store the best metrics for this subset size\n",
    "    results[\"subset_size\"].append(subset_size)\n",
    "    results[\"best_epoch\"].append(best_epoch)\n",
    "    results[\"best_accuracy\"].append(best_acc)\n",
    "    results[\"best_precision\"].append(precision)\n",
    "    results[\"best_recall\"].append(recall)\n",
    "    results[\"best_f1_score\"].append(f1_score)\n",
    "    results[\"all_preds\"].append(all_preds)\n",
    "    results[\"all_labels\"].append(all_labels)\n",
    "    results[\"best_model\"].append(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9d37a",
   "metadata": {},
   "source": [
    "### Obtain metrics for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4c56725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results for each subset size:\n",
      "Subset Size 10700:\n",
      "  - Best Epoch: 10\n",
      "  - Best Accuracy: 49.07%\n",
      "  - Best Precision: 49.41%\n",
      "  - Best Recall: 46.51%\n",
      "  - Best F1-Score: 47.92%\n",
      "\n",
      "Subset Size 21400:\n",
      "  - Best Epoch: 90\n",
      "  - Best Accuracy: 48.34%\n",
      "  - Best Precision: 49.83%\n",
      "  - Best Recall: 46.93%\n",
      "  - Best F1-Score: 48.34%\n",
      "\n",
      "Subset Size 53500:\n",
      "  - Best Epoch: 0\n",
      "  - Best Accuracy: 49.99%\n",
      "  - Best Precision: 49.93%\n",
      "  - Best Recall: 46.16%\n",
      "  - Best F1-Score: 47.97%\n",
      "\n",
      "Subset Size 107000:\n",
      "  - Best Epoch: 0\n",
      "  - Best Accuracy: 48.53%\n",
      "  - Best Precision: 49.26%\n",
      "  - Best Recall: 46.79%\n",
      "  - Best F1-Score: 47.99%\n",
      "\n",
      "Subset Size 213907:\n",
      "  - Best Epoch: 0\n",
      "  - Best Accuracy: 48.88%\n",
      "  - Best Precision: 49.86%\n",
      "  - Best Recall: 47.04%\n",
      "  - Best F1-Score: 48.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best epoch after all subsets have been processed\n",
    "print(\"\\nFinal Results for each subset size:\")\n",
    "for i, subset_size in enumerate(results[\"subset_size\"]):\n",
    "    print(f\"Subset Size {subset_size}:\")\n",
    "    print(f\"  - Best Epoch: {results['best_epoch'][i]}\")\n",
    "    print(f\"  - Best Accuracy: {results['best_accuracy'][i] * 100:.2f}%\")\n",
    "    print(f\"  - Best Precision: {results['best_precision'][i] * 100:.2f}%\")\n",
    "    print(f\"  - Best Recall: {results['best_recall'][i] * 100:.2f}%\")\n",
    "    print(f\"  - Best F1-Score: {results['best_f1_score'][i] * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd2d217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eval_metrics(predicted_labels, labels, unique_labels):\n",
    "    \n",
    "    #Accuracy\n",
    "    accuracy = np.mean(predicted_labels == labels)\n",
    "    \n",
    "    # Recall y precision por clase\n",
    "    class_precision = []\n",
    "    class_recall = []\n",
    "\n",
    "    for label in unique_labels: #Para cada clase\n",
    "        #Comprobamos que hay etiquetas de esta clase, si no las hay ponemos nan en precision y en recall\n",
    "        presence_aid = np.sum(labels == label)\n",
    "        if presence_aid == 0:\n",
    "            Precision = np.nan \n",
    "            Recall = np.nan\n",
    "        else: \n",
    "            VP = np.sum((predicted_labels == label) & (labels == label)) #Number of correct detections \n",
    "            FP = np.sum((predicted_labels == label) & (labels != label)) #Number of incorrect detections\n",
    "            FN = np.sum((predicted_labels != label) & (labels == label)) #Está predicha como otra etiqueta cuando pertenece a esta etiqueta\n",
    "\n",
    "            Precision = VP/(VP+FP)\n",
    "            Recall = VP/(VP+FN)\n",
    "\n",
    "        class_precision.append(Precision)\n",
    "        class_recall.append(Recall)\n",
    "    \n",
    "    precision = np.nanmean(class_precision)\n",
    "    recall = np.nanmean(class_recall)\n",
    "    \n",
    "    f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "        \n",
    "    return accuracy, class_precision, class_recall, precision, recall, f1_score\n",
    "\n",
    "def metrics_per_patient(outputs, labels, patients, paciente_indices): \n",
    "    \n",
    "    predicted_labels = outputs\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    acc_total = []\n",
    "    recall_total = []\n",
    "    precision_total = []\n",
    "    f1_score_total = []\n",
    "    class_recall_total = np.empty((len(patients), len(unique_labels)))\n",
    "    class_precision_total = np.empty((len(patients), len(unique_labels)))\n",
    "\n",
    "    for i in range(len(patients)):\n",
    "        pred = np.array(predicted_labels[paciente_indices[i]:paciente_indices[i+1]])\n",
    "        labels_GT = np.array(labels[paciente_indices[i]:paciente_indices[i+1]])\n",
    "        \n",
    "        accuracy, class_precision, class_recall, precision, recall, f1_score = compute_eval_metrics(pred, labels_GT, unique_labels)\n",
    "        \n",
    "        acc_total.append(accuracy)\n",
    "        recall_total.append(recall)\n",
    "        precision_total.append(precision)\n",
    "        f1_score_total.append(f1_score)\n",
    "        \n",
    "        class_recall_total[i, :] = class_recall\n",
    "        class_precision_total[i, :] = class_precision\n",
    "    \n",
    "    acc_patients = sum(acc_total)/len(acc_total)\n",
    "    acc_patients_std = np.std(np.array(acc_total))\n",
    "    \n",
    "    recall_patients = sum(recall_total)/len(recall_total)\n",
    "    recall_patients_std = np.std(np.array(recall_total))\n",
    "    \n",
    "    precision_patients = sum(precision_total)/len(precision_total)\n",
    "    precision_patients_std = np.std(np.array(precision_total))\n",
    "    \n",
    "    f1_score_patients = sum(f1_score_total)/len(f1_score_total)\n",
    "    f1_score_patients_std = np.std(np.array(f1_score_total))\n",
    "    \n",
    "    \n",
    "    class_recall_patients_mean = np.nanmean(class_recall_total,0)\n",
    "    class_recall_patients_std = np.nanstd(class_recall_total,0)\n",
    "    class_precision_patients_mean = np.nanmean(class_precision_total,0)\n",
    "    class_precision_patients_std = np.nanstd(class_precision_total,0)\n",
    "    \n",
    "    recall_porfase = np.nanmean(class_recall_patients_mean)\n",
    "    recall_porfase_std = np.nanstd(class_recall_patients_mean)\n",
    "    precision_porfase = np.nanmean(class_precision_patients_mean)\n",
    "    precision_porfase_std = np.nanstd(class_precision_patients_mean)\n",
    "    f1_porfase = (2*recall_porfase*precision_porfase)/(recall_porfase+precision_porfase)\n",
    "    f1_porfase_std = (2*recall_porfase_std*precision_porfase_std)/(recall_porfase_std+precision_porfase_std)\n",
    "\n",
    "    return (acc_patients, acc_patients_std, recall_patients, recall_patients_std, precision_patients, \n",
    "            precision_patients_std, f1_score_patients, f1_score_patients_std, class_recall_patients_mean, \n",
    "            class_recall_patients_std, class_precision_patients_mean, class_precision_patients_std,recall_porfase,\n",
    "            recall_porfase_std, precision_porfase,precision_porfase_std, f1_porfase, f1_porfase_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0614956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_patients_index(ruta_csv):\n",
    "    data = pd.read_csv(ruta_csv, header=0)\n",
    "    paciente_indices = []\n",
    "    paciente_actual = 0\n",
    "    patients = []\n",
    "    for indice, fila in data.iterrows():\n",
    "        id_paciente = fila[\"id\"].split(\"_\")[0]  # Obtener el número de paciente\n",
    "        if id_paciente != paciente_actual:\n",
    "            patients.append(id_paciente)\n",
    "            inicio = indice\n",
    "            paciente_indices.append(inicio)\n",
    "            paciente_actual = id_paciente\n",
    "    \n",
    "    paciente_indices.append(len(data))\n",
    "    return patients, paciente_indices\n",
    "\n",
    "patients, paciente_indices = obtain_patients_index(\"/home/val_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e075462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS EN TEST: \n",
      "- Accuracy: 0.57 ± 0.01\n",
      "- Recall: 0.48 ± 0.01\n",
      "- Precision: 0.50 ± 0.01\n",
      "- F1_Score: 0.49 ± 0.01\n",
      "- Class recall: [0.93403614 0.33271513 0.17934803 0.58161686 0.11415918 0.75949306\n",
      " 0.46424397] ± [0.01218345 0.05605436 0.02223143 0.01305892 0.02228987 0.0099106\n",
      " 0.02709561]\n",
      "- Class precision: [0.93707882 0.42987302 0.25640109 0.48340517 0.29321408 0.688192\n",
      " 0.39924266] ± [0.01632714 0.04151991 0.03437595 0.01350279 0.02253073 0.01163821\n",
      " 0.0232853 ]\n",
      "RESULTADOS POR FASE\n",
      "- Recall: 0.48 ± 0.28\n",
      "- Precision: 0.50 ± 0.22\n",
      "- F1-Score: 0.49 ± 0.25\n",
      "\n",
      "\n",
      "RESULTADOS EN TEST: \n",
      "- Accuracy: 0.59 ± 0.01\n",
      "- Recall: 0.51 ± 0.01\n",
      "- Precision: 0.52 ± 0.01\n",
      "- F1_Score: 0.51 ± 0.01\n",
      "- Class recall: [0.94485693 0.39138155 0.19677174 0.59405454 0.17029827 0.76664377\n",
      " 0.48714034] ± [0.01420837 0.03814886 0.02163269 0.01638961 0.02074671 0.01080339\n",
      " 0.01733291]\n",
      "- Class precision: [0.92095196 0.45283816 0.28395754 0.5083218  0.35411762 0.69729482\n",
      " 0.43476719] ± [0.0140898  0.02439012 0.02751902 0.01543957 0.02882622 0.00539089\n",
      " 0.01648643]\n",
      "RESULTADOS POR FASE\n",
      "- Recall: 0.51 ± 0.26\n",
      "- Precision: 0.52 ± 0.20\n",
      "- F1-Score: 0.51 ± 0.23\n",
      "\n",
      "\n",
      "RESULTADOS EN TEST: \n",
      "- Accuracy: 0.58 ± 0.01\n",
      "- Recall: 0.49 ± 0.01\n",
      "- Precision: 0.51 ± 0.01\n",
      "- F1_Score: 0.50 ± 0.01\n",
      "- Class recall: [0.93658989 0.33668006 0.17038928 0.55528818 0.15204701 0.77989831\n",
      " 0.48308714] ± [0.01399547 0.04594811 0.0193432  0.01652173 0.02780674 0.00816066\n",
      " 0.02405087]\n",
      "- Class precision: [0.94963844 0.46253099 0.26632708 0.47970227 0.31443165 0.68948499\n",
      " 0.42663794] ± [0.01051116 0.04797205 0.03336877 0.01470955 0.03944049 0.00652701\n",
      " 0.01886628]\n",
      "RESULTADOS POR FASE\n",
      "- Recall: 0.49 ± 0.27\n",
      "- Precision: 0.51 ± 0.22\n",
      "- F1-Score: 0.50 ± 0.24\n",
      "\n",
      "\n",
      "RESULTADOS EN TEST: \n",
      "- Accuracy: 0.59 ± 0.01\n",
      "- Recall: 0.51 ± 0.01\n",
      "- Precision: 0.52 ± 0.01\n",
      "- F1_Score: 0.51 ± 0.01\n",
      "- Class recall: [0.94668631 0.40379634 0.16386301 0.58199045 0.18594471 0.76898778\n",
      " 0.49136646] ± [0.00992674 0.04163784 0.01383297 0.01696665 0.02339199 0.00858024\n",
      " 0.0262748 ]\n",
      "- Class precision: [0.93524852 0.48876398 0.27398895 0.47154075 0.3411509  0.69710049\n",
      " 0.44498308] ± [0.00996041 0.0441471  0.02312184 0.01336756 0.03288364 0.00642896\n",
      " 0.01675413]\n",
      "RESULTADOS POR FASE\n",
      "- Recall: 0.51 ± 0.27\n",
      "- Precision: 0.52 ± 0.21\n",
      "- F1-Score: 0.51 ± 0.23\n",
      "\n",
      "\n",
      "RESULTADOS EN TEST: \n",
      "- Accuracy: 0.59 ± 0.01\n",
      "- Recall: 0.48 ± 0.01\n",
      "- Precision: 0.52 ± 0.01\n",
      "- F1_Score: 0.50 ± 0.01\n",
      "- Class recall: [0.94815329 0.26466697 0.17159572 0.56871514 0.13303622 0.78460483\n",
      " 0.51316463] ± [0.00754874 0.02859561 0.02199682 0.02184364 0.02358844 0.00969687\n",
      " 0.02710812]\n",
      "- Class precision: [0.93477796 0.48880357 0.28782088 0.48476698 0.3283058  0.68719618\n",
      " 0.42690799] ± [0.00952456 0.03449619 0.02889618 0.015919   0.04677496 0.00813846\n",
      " 0.01848599]\n",
      "RESULTADOS POR FASE\n",
      "- Recall: 0.48 ± 0.29\n",
      "- Precision: 0.52 ± 0.21\n",
      "- F1-Score: 0.50 ± 0.24\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, subset_size in enumerate(results[\"subset_size\"]):\n",
    "    \n",
    "    labels_pred = results['all_preds'][i]\n",
    "    labels_gt = results['all_labels'][i]\n",
    "\n",
    "    (acc_patients, acc_patients_std, recall_patients, recall_patients_std, precision_patients, \n",
    "     precision_patients_std, f1_score_patients, f1_score_patients_std, class_recall_patients_mean, \n",
    "     class_recall_patients_std, class_precision_patients_mean, class_precision_patients_std, recall_porfase, \n",
    "     recall_porfase_std, precision_porfase, precision_porfase_std, f1_porfase, f1_porfase_std) = metrics_per_patient( labels_pred, labels_gt, patients, paciente_indices)\n",
    "    \n",
    "    print('RESULTADOS EN TEST: ')\n",
    "    print(f'- Accuracy: {acc_patients:.2f} ± {acc_patients_std:.2f}')\n",
    "    print(f'- Recall: {recall_patients:.2f} ± {recall_patients_std:.2f}')\n",
    "    print(f'- Precision: {precision_patients:.2f} ± {precision_patients_std:.2f}')\n",
    "    print(f'- F1_Score: {f1_score_patients:.2f} ± {f1_score_patients_std:.2f}')\n",
    "    print(f'- Class recall: {class_recall_patients_mean} ± {class_recall_patients_std}')\n",
    "    print(f'- Class precision: {class_precision_patients_mean} ± {class_precision_patients_std}')\n",
    "\n",
    "    print('RESULTADOS POR FASE')\n",
    "    print(f'- Recall: {recall_porfase:.2f} ± {recall_porfase_std:.2f}')\n",
    "    print(f'- Precision: {precision_porfase:.2f} ± {precision_porfase_std:.2f}')\n",
    "    print(f'- F1-Score: {f1_porfase:.2f} ± {f1_porfase_std:.2f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f963a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model \n",
    "subset = 10700\n",
    "best_model = results[subset][\"best_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a85b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_label(model, dataset, dataloader):\n",
    "    numClasses = 7\n",
    "    model.eval()   # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    numSamples = len(dataset)  # Size of the dataset\n",
    "    outputs_m = np.zeros((numSamples, numClasses), dtype=np.float)\n",
    "    labels_m = np.zeros((numSamples,), dtype=np.int)\n",
    "    contSamples = 0\n",
    "\n",
    "    # Iterate over the data\n",
    "    for inputs, labels in dataloader:\n",
    "        batchSize = inputs.size(0)\n",
    "\n",
    "        # Move data to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Apply softmax to the output\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs_m[contSamples:contSamples + batchSize, ...] = outputs.cpu().numpy()\n",
    "            labels_m[contSamples:contSamples + batchSize] = labels.cpu().numpy()\n",
    "            contSamples += batchSize\n",
    "\n",
    "    return outputs_m, labels_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4630837",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_val, labels_val = obtain_label(best_model,val_img_data,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "predicted_labels_val = np.argmax(outputs_val, axis=1)\n",
    "cm = confusion_matrix(labels_val, predicted_labels_val, normalize = 'true')\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Confusion Matrix for SimCLR Model for best model')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
