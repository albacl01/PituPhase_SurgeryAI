{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5befc28d",
   "metadata": {},
   "source": [
    "# Downstream Evaluation of SSL Representations\n",
    "\n",
    "This notebook performs the **linear evaluation protocol** on the frozen feature representations learned by our self-supervised models (SimCLR and BYOL) for the surgical phase recognition task.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Load Pretrained Features**  \n",
    "   - Read in the feature embeddings extracted from each frame by the pretrained encoder (with and without attention).\n",
    "\n",
    "2. **Prepare Labels and Splits**  \n",
    "   - Load the corresponding per-frame CSV label files.\n",
    "\n",
    "3. **Train Linear Classifier**  \n",
    "   - For each SSL model (SimCLR, BYOL, with/without attention), train a simple logistic regression or single-layer MLP on the training embeddings.\n",
    "   - Keep the encoder frozen; only the classifier weights are updated.\n",
    "\n",
    "4. **Evaluate Performance**  \n",
    "   - Compute standard metrics:  \n",
    "     - **Accuracy** over full video sequences  \n",
    "     - **Macro-averaged Precision, Recall, and F1-score** across the 7 surgical phases  \n",
    "     - **Confusion Matrix** to inspect class-wise performance  \n",
    "   - Optionally, assess robustness by retraining/evaluating on reduced label subsets (50%, 25%, 10%, 5%).\n",
    "\n",
    "5. **Visualize Results**  \n",
    "   - Plot F1-score vs. fraction of labels used  \n",
    "   - Display confusion matrices side-by-side for SimCLR vs. BYOL, and attention vs. no-attention configurations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d100951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of workers: 16\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from skimage import io, transform, util\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f30a57",
   "metadata": {},
   "source": [
    "## 2. Downstream task (Fine-tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cf875",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f80394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PituDataset(Dataset):\n",
    "    \"\"\"Pituitary Endoscopy dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, maxSize=0, unlabeled=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            maxSize (int, optional): Maximum size of the dataset (number of samples).\n",
    "            unlabeled (bool, optional): If True, ignore labels.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file, header=0, dtype={'id': str, 'label': int})\n",
    "        \n",
    "        if maxSize > 0:\n",
    "            newDatasetSize = maxSize  # maxSize samples (Parameter to select a specific number of images)\n",
    "            idx = np.random.RandomState(seed=42).permutation(range(len(self.dataset)))\n",
    "            reduced_dataset = self.dataset.iloc[idx[0:newDatasetSize]]\n",
    "            self.dataset = reduced_dataset.reset_index(drop=True)\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.unlabeled = unlabeled\n",
    "        self.classes = ['Desconocida', 'Preparacion colgajo', 'Etmoidectomia', 'Apertura selar', \n",
    "                        'Apertura dural', 'Reseccion tumoral', 'Cierre']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # Read the image\n",
    "        img_name = os.path.join(self.img_dir, self.dataset.id[idx] + '.png')\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        if self.unlabeled:\n",
    "            sample = {'image': image, 'label': np.int64(-1)}  # Use -1 to indicate unlabeled, keep datatype\n",
    "        else:\n",
    "            sample = {'image': image, 'label': self.dataset.label[idx].astype(dtype=np.long)}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f50e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "class Rescale(object):\n",
    "    \"\"\"Re-scale image to a predefined size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): The desired size. If it is a tuple, output is the output_size. \n",
    "        If it is an int, the smallest dimension will be the output_size\n",
    "            a we will keep fixed the original aspect ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'],sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'label' : label}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays into pytorch tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # Cambiamos los ejes\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        label=torch.tensor(label,dtype=torch.long)\n",
    "        \n",
    "        return {'image':image,\n",
    "                'label':label}\n",
    "    \n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize data by subtracting means and dividing by standard deviations.\n",
    "\n",
    "    Args:\n",
    "        mean_vec: Vector with means. \n",
    "        std_vec: Vector with standard deviations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean,std):\n",
    "      \n",
    "        assert len(mean)==len(std),'Length of mean and std vectors is not the same'\n",
    "        self.mean = np.array(mean)\n",
    "        self.std = np.array(std)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'],sample['label']\n",
    "        c, h, w = image.shape\n",
    "        assert c==len(self.mean), 'Length of mean and image is not the same' \n",
    "        dtype = image.dtype\n",
    "        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n",
    "        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n",
    "        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    \n",
    "        \n",
    "        return {'image': image, 'label' : label}\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crop the central area of the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tupla or int): Crop size. If int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        rem_h = h - new_h\n",
    "        rem_w = w - new_w\n",
    "        \n",
    "        if h>new_h:\n",
    "            top = int(rem_h/2)\n",
    "        else:\n",
    "            top=0\n",
    "            \n",
    "        if w>new_w: \n",
    "            left = int(rem_w/2)\n",
    "        else:\n",
    "            left = 0\n",
    "            \n",
    "        image = image[top: top + new_h,\n",
    "                     left: left + new_w]\n",
    "\n",
    "\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7fa99",
   "metadata": {},
   "source": [
    "Load the data with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbb306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 213907\n",
      "Number of test examples: 56431\n"
     ]
    }
   ],
   "source": [
    "pixel_mean = [0.312, 0.120, 0.117]\n",
    "pixel_std = [0.280, 0.158, 0.160]\n",
    "\n",
    "img_transforms = transforms.Compose([CenterCrop((256, 320)),\n",
    "                                     Rescale((224,224)),\n",
    "                                     ToTensor(),\n",
    "                                     Normalize(mean=pixel_mean, std=pixel_std)])\n",
    "\n",
    "train_img_data = PituDataset(csv_file=\"/home/train_set.csv\",\n",
    "                                      root_dir='/home',\n",
    "                                      #maxSize=100000,\n",
    "                                      transform=img_transforms)\n",
    "\n",
    "val_img_data = PituDataset(csv_file=\"/home/val_set.csv\",\n",
    "                            root_dir='/home',\n",
    "                            transform=img_transforms)\n",
    "\n",
    "\n",
    "print(\"Number of training examples:\", len(train_img_data))\n",
    "print(\"Number of test examples:\", len(val_img_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede8cc3",
   "metadata": {},
   "source": [
    "## Load the pre-trained model (SimCLR or BYOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7247e6",
   "metadata": {},
   "source": [
    "### SimCLR implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b899285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        \n",
    "        # Base model f(.): ResNet-50 \n",
    "        self.convnet = torchvision.models.resnet50()\n",
    "        in_features = self.convnet.fc.in_features  # 2048 for ResNet-50 (this is vector h)\n",
    "\n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
    "        # this is the projection head: 2048 → 4 * hidden_dim → hidden_dim\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 4 * hidden_dim),  # Linear(2048, 4*hidden_dim) (input h)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)  # Linear(4*hidden_dim, hidden_dim) (output z)\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "        \n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        imgs = torch.cat(batch['image'], dim=0) # doesn't use labels\n",
    "        imgs = imgs.to(device=device, dtype=torch.float) \n",
    "        \n",
    "        # Encode all images\n",
    "        feats = self.convnet(imgs) # this is vector z\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1) \n",
    "        \n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        \n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
    "        \n",
    "        # InfoNCE loss\n",
    "        cos_sim = cos_sim / self.hparams.temperature\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "        \n",
    "        ###--- Logg metrics ---###\n",
    "        \n",
    "        # Logging loss\n",
    "        self.log(mode+'_loss', nll)\n",
    "        \n",
    "        # Get ranking position of positive example\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
    "                              cos_sim.masked_fill(pos_mask, -9e15)], \n",
    "                             dim=-1)\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
    "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
    "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
    "        \n",
    "        return nll\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b9665",
   "metadata": {},
   "source": [
    "### BYOL Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    BYOL (Bootstrap Your Own Latent) implementation.\n",
    "\n",
    "    This class defines the architecture and training process for a self-supervised learning\n",
    "    model, allowing it to learn useful representations without using labeled data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, projection_size, lr, momentum, weight_decay, moving_average_decay):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim (int): The size of the hidden vector in the MLPs of the student and teacher projection heads.\n",
    "            projection_size (int): The size of the output vector from the projection head (dimension of the embedding space).\n",
    "            lr (float): Learning rate for the optimizer.\n",
    "            momentum (float): Momentum parameter for the SGD optimizer.\n",
    "            weight_decay (float): Weight decay for L2 regularization.\n",
    "            moving_average_decay (float): Decay factor for the exponential moving average used to update the teacher model. e.g. 0.99\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Base encoder f(.): ResNet-50\n",
    "        self.backbone = torchvision.models.resnet50()\n",
    "        in_features = self.backbone.fc.in_features  # 2048 for ResNet-50 (this is vector h)\n",
    "        self.backbone.fc = nn.Identity()  # Remove the final classification layer to get the feature vector\n",
    "        \n",
    "        # Projection head g(·) --> consists of Linear->BN->ReLU->Linear\n",
    "        self.student_projector = MLP(in_features, hidden_dim, projection_size) #(2048,4096,512)\n",
    "        \n",
    "        # Prediction head q(·)\n",
    "        self.student_predictor = MLP(projection_size, hidden_dim, projection_size)  #(512,4096,512) (output vector q)\n",
    "        \n",
    "        # Teacher model\n",
    "        self.teacher_projector = copy.deepcopy(self.student_projector)\n",
    "        \n",
    "        # EMA parameters\n",
    "        self.moving_average_decay = moving_average_decay\n",
    "        \n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        '''optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        '''\n",
    "        optimizer = optim.SGD(self.parameters(),\n",
    "                              lr=self.hparams.lr,\n",
    "                              weight_decay=self.hparams.weight_decay,\n",
    "                              momentum=self.hparams.momentum)\n",
    "        \n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_moving_average(self):\n",
    "        \"\"\"\n",
    "        Updates the weights of the teacher model as a moving average of the student model's weights.\n",
    "        \"\"\"\n",
    "        for student_params, teacher_params in zip(self.student_projector.parameters(), self.teacher_projector.parameters()):\n",
    "            teacher_params.data = teacher_params.data * self.moving_average_decay + (1. - self.moving_average_decay) * student_params.data\n",
    "          \n",
    "        \n",
    "    def initializes_target_network(self):\n",
    "        '''\n",
    "        Initializes the target (teacher) network with the same weights as the student model.\n",
    "        Ensures the teacher's parameters do not require gradient updates.\n",
    "        '''\n",
    "        \n",
    "        for student_params, teacher_params in zip(self.student_projector.parameters(), self.teacher_projector.parameters()):\n",
    "            teacher_params.data.copy_(student_params.data)  # initialize\n",
    "            teacher_params.requires_grad = False  # not update by gradient\n",
    "            \n",
    "    \n",
    "    def on_train_start(self):\n",
    "        # Initialize the teacher network at the start of training\n",
    "        self.initializes_target_network()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass through the student network and student projector\n",
    "        '''\n",
    "        features = self.backbone(x)\n",
    "        student_projection = self.student_projector(features)\n",
    "        student_prediction = self.student_predictor(student_projection)\n",
    "        return student_prediction\n",
    "    \n",
    "\n",
    "    def shared_step(self, img1, img2):\n",
    "    \n",
    "        # get student projections: backbone + MLP projection head\n",
    "        feats1 = self.backbone(img1) #this is h\n",
    "        feats2 = self.backbone(img2)\n",
    "        \n",
    "        student_proj1 = self.student_projector(feats1) #this is g\n",
    "        student_proj2 = self.student_projector(feats2)\n",
    "\n",
    "        # Apply the predictor MLP to the student's projections\n",
    "        student_pred1 = self.student_predictor(student_proj1) # this is q\n",
    "        student_pred2 = self.student_predictor(student_proj2)\n",
    "\n",
    "        # Get teacher projections (no gradient updates)\n",
    "        with torch.no_grad():\n",
    "            # teacher processes the images and makes projections: backbone + MLP\n",
    "            teacher_proj1 = self.teacher_projector(feats1) \n",
    "            teacher_proj2 = self.teacher_projector(feats2)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = L2_loss(student_pred1, teacher_proj2)\n",
    "        loss += L2_loss(student_pred2, teacher_proj1)        \n",
    "\n",
    "        return loss.mean() #loss = (loss1 + loss2).mean()\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img1, img2 = batch['image'][0], batch['image'][1]\n",
    "        img1 = img1.to(device=device, dtype=torch.float)\n",
    "        img2 = img2.to(device=device, dtype=torch.float)\n",
    "\n",
    "        loss = self.shared_step(img1, img2)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        # Update the teacher model\n",
    "        self.update_moving_average()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img1, img2 = batch['image'][0], batch['image'][1]\n",
    "        img1 = img1.to(device=device, dtype=torch.float)\n",
    "        img2 = img2.to(device=device, dtype=torch.float)\n",
    "\n",
    "        loss = self.shared_step(img1, img2)\n",
    "        self.log('val_loss', loss)\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2781d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimCLR(\n",
      "  (convnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the saved .pt model\n",
    "model_path = '/home/simclr_models/simclr_model.pt'\n",
    "\n",
    "# Initialize the model\n",
    "loaded_model = SimCLR(max_epochs=5, hidden_dim=64, lr=5e-4, temperature=0.07, weight_decay=1e-4)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d67d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard everything except the encoder\n",
    "model=loaded_model.convnet\n",
    "\n",
    "# eliminate the last classification layer\n",
    "encoder = nn.Sequential(*list(model.children())[:-1])    \n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadce866",
   "metadata": {},
   "source": [
    "## Encode images\n",
    "Next, we implement a small function to encode the images in our datasets. The output representations are then used as inputs to the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76e589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(encoder, dataset):\n",
    "    \"\"\"\n",
    "    Extracts features from the encoder for a given dataset and returns a TensorDataset.\n",
    "    \n",
    "    Args:\n",
    "        encoder (nn.Module): Pre-trained encoder model without the final classification layer.\n",
    "        dataset (Dataset): Dataset for which features need to be extracted.\n",
    "        \n",
    "    Returns:\n",
    "        TensorDataset: A dataset containing the extracted features and corresponding labels.\n",
    "    \"\"\"\n",
    "    # Set encoder to evaluation mode and move to the correct device\n",
    "    encoder.eval()\n",
    "    encoder = encoder.float()  # Ensure the encoder uses float precision\n",
    "    encoder.to(device)\n",
    "    \n",
    "    feats = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Prepare the data loader\n",
    "    data_loader = DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
    "\n",
    "    # Get the features from the pre-trained model\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        imgs = batch['image'].to(device, dtype=torch.float)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = encoder(imgs)\n",
    "            \n",
    "            feats.append(features.detach().cpu())\n",
    "            labels_list.append(labels.detach().cpu())\n",
    "    \n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "\n",
    "    return feats, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a00132",
   "metadata": {},
   "source": [
    "The original data loaders `train_loader, val_loader` are used to extract features from the images using a pre-trained encoder `encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34a2277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([213907, 2048, 1, 1]) torch.Size([213907])\n",
      "Testing data shape: torch.Size([56431, 2048, 1, 1]) torch.Size([56431])\n"
     ]
    }
   ],
   "source": [
    "# The function extracts features for each input image batch from the encoder\n",
    "x_train, y_train = prepare_data_features(encoder, train_img_data)\n",
    "x_test, y_test = prepare_data_features(encoder, val_img_data)\n",
    "\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711c7892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([213907, 2048]) torch.Size([213907])\n",
      "Testing data shape: torch.Size([56431, 2048]) torch.Size([56431])\n"
     ]
    }
   ],
   "source": [
    "# checks if the feature tensor has more than two dimensions: [N, feature_dim, height, width]\n",
    "if len(x_test.shape) > 2:\n",
    "    x_train = torch.mean(x_train, dim=[2, 3]) # reduce the shape of the features to [N, feature_dim]\n",
    "    x_test = torch.mean(x_test, dim=[2, 3])\n",
    "\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33020bb1",
   "metadata": {},
   "source": [
    "After feature extraction, the features `x_train, x_test` are standardized for better training of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bb9629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the extracted features: mean of 0 and a standard deviation of 1\n",
    "scaler = preprocessing.StandardScaler()  \n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train).astype(np.float32) # convert to float32 \n",
    "x_test = scaler.transform(x_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34250f9b",
   "metadata": {},
   "source": [
    "New data loaders are created to work with the scaled features instead of raw image data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ecddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    '''- Input: Takes feature vectors (X_train and X_test) and corresponding labels (y_train and y_test).\n",
    "        \n",
    "       - Purpose: Converts the feature arrays and their labels into TensorDataset objects. \n",
    "       This allows the features and labels to be combined as tensors, which is the format that PyTorch expects.'''\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    val_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c910f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_data_loaders_from_arrays(torch.from_numpy(x_train), y_train, torch.from_numpy(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137441a0",
   "metadata": {},
   "source": [
    "These new loaders `train_loader, val_loader` will be used to train a simple classifier (e.g., logistic regression) on the extracted, standardized features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7ea37",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Now apply the extracted characteristics on a supervised downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e86e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4834c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eval_metrics(predicted_labels, labels):\n",
    "    #Accuracy\n",
    "    accuracy = np.mean(predicted_labels == labels)\n",
    "    \n",
    "    # Recall y precision por clase\n",
    "    class_precision = []\n",
    "    class_recall = []\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    \n",
    "    for label in unique_labels: #Para cada clase\n",
    "        VP = np.sum((predicted_labels == label) & (labels == label)) #Number of correct detections \n",
    "        FP = np.sum((predicted_labels == label) & (labels != label)) #Number of incorrect detections\n",
    "        FN = np.sum((predicted_labels != label) & (labels == label)) \n",
    "\n",
    "        Precision = VP/(VP+FP)\n",
    "        Recall = VP/(VP+FN)\n",
    "\n",
    "        class_precision.append(Precision)\n",
    "        class_recall.append(Recall)\n",
    "    \n",
    "    precision = np.mean(class_precision)\n",
    "    recall = np.mean(class_recall)\n",
    "    \n",
    "    f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "        \n",
    "    return accuracy, class_precision, class_recall, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f4ef5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model, input_dim=2048, output_dim=7\n",
    "output_feature_dim = loaded_model.convnet.fc[0].in_features # 2048\n",
    "\n",
    "logreg = LogisticRegression(output_feature_dim, 7) #(2048,7)\n",
    "logreg = logreg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c109d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion - Weighted CE loss   \n",
    "num_etiquetas = [9378, 10254, 37582, 45600, 20624, 50000, 46586]    \n",
    "\n",
    "weights = []\n",
    "for num in num_etiquetas:\n",
    "    weight_for_class_i = sum(num_etiquetas) / num\n",
    "    weights.append(weight_for_class_i)\n",
    "    \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weights=weights)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(logreg.parameters(), lr=0.0001, weight_decay = 0.01)\n",
    "\n",
    "# Define the loss function\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store metrics for plotting\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Variables for best epoch\n",
    "eval_every_n_epochs = 10\n",
    "best_acc = 0\n",
    "best_epoch = -1\n",
    "\n",
    "# time\n",
    "since = time.time()\n",
    "n=100\n",
    "for epoch in range(n):\n",
    "    \n",
    "    # Training loop\n",
    "    logreg.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = logreg(imgs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluation loop every 10 epochs\n",
    "    if epoch % eval_every_n_epochs == 0:\n",
    "        logreg.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = logreg(imgs)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                # Store predictions and labels for metric calculation\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Convert lists to numpy arrays for metric calculations\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # Compute metrics\n",
    "        unique_labels = np.unique(all_labels)\n",
    "        accuracy, class_precision, class_recall, precision, recall, f1_score = compute_eval_metrics(all_preds, all_labels)\n",
    "        \n",
    "        # Store evaluation metrics\n",
    "        val_accuracies.append(accuracy)\n",
    "        val_precisions.append(precision)\n",
    "        val_recalls.append(recall)\n",
    "        val_f1_scores.append(f1_score)\n",
    "        \n",
    "        # Deep copy the best model\n",
    "        if f1_score > best_acc:\n",
    "            best_acc = f1_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            \n",
    "        # Print the results\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        print(f\"  - Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"  - Precision: {precision * 100:.2f}%\")\n",
    "        print(f\"  - Recall: {recall * 100:.2f}%\")\n",
    "        print(f\"  - F1-Score: {f1_score * 100:.2f}%\\n\")\n",
    "        \n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best model in epoch {:d} at val F1-Score: {:.4f}'.format(best_epoch, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation metrics\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(range(0, n, eval_every_n_epochs), val_accuracies, label='Validation Accuracy')\n",
    "plt.plot(range(0, n, eval_every_n_epochs), val_precisions, label='Validation Precision')\n",
    "plt.plot(range(0, n, eval_every_n_epochs), val_recalls, label='Validation Recall')\n",
    "plt.plot(range(0, n, eval_every_n_epochs), val_f1_scores, label='Validation F1-Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation Metrics')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bd8f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_per_patient(outputs, labels, patients, paciente_indices): \n",
    "    \n",
    "    predicted_labels = outputs\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    acc_total = []\n",
    "    recall_total = []\n",
    "    precision_total = []\n",
    "    f1_score_total = []\n",
    "    class_recall_total = np.empty((len(patients), len(unique_labels)))\n",
    "    class_precision_total = np.empty((len(patients), len(unique_labels)))\n",
    "\n",
    "    for i in range(len(patients)):\n",
    "        pred = np.array(predicted_labels[paciente_indices[i]:paciente_indices[i+1]])\n",
    "        labels_GT = np.array(labels[paciente_indices[i]:paciente_indices[i+1]])\n",
    "        \n",
    "        accuracy, class_precision, class_recall, precision, recall, f1_score = compute_eval_metrics(pred, labels_GT)\n",
    "        \n",
    "        acc_total.append(accuracy)\n",
    "        recall_total.append(recall)\n",
    "        precision_total.append(precision)\n",
    "        f1_score_total.append(f1_score)\n",
    "        \n",
    "        class_recall_total[i, :] = class_recall\n",
    "        class_precision_total[i, :] = class_precision\n",
    "    \n",
    "    acc_patients = sum(acc_total)/len(acc_total)\n",
    "    acc_patients_std = np.std(np.array(acc_total))\n",
    "    \n",
    "    recall_patients = sum(recall_total)/len(recall_total)\n",
    "    recall_patients_std = np.std(np.array(recall_total))\n",
    "    \n",
    "    precision_patients = sum(precision_total)/len(precision_total)\n",
    "    precision_patients_std = np.std(np.array(precision_total))\n",
    "    \n",
    "    f1_score_patients = sum(f1_score_total)/len(f1_score_total)\n",
    "    f1_score_patients_std = np.std(np.array(f1_score_total))\n",
    "    \n",
    "    \n",
    "    class_recall_patients_mean = np.nanmean(class_recall_total,0)\n",
    "    class_recall_patients_std = np.nanstd(class_recall_total,0)\n",
    "    class_precision_patients_mean = np.nanmean(class_precision_total,0)\n",
    "    class_precision_patients_std = np.nanstd(class_precision_total,0)\n",
    "    \n",
    "    recall_porfase = np.nanmean(class_recall_patients_mean)\n",
    "    recall_porfase_std = np.nanstd(class_recall_patients_mean)\n",
    "    precision_porfase = np.nanmean(class_precision_patients_mean)\n",
    "    precision_porfase_std = np.nanstd(class_precision_patients_mean)\n",
    "    f1_porfase = (2*recall_porfase*precision_porfase)/(recall_porfase+precision_porfase)\n",
    "    f1_porfase_std = (2*recall_porfase_std*precision_porfase_std)/(recall_porfase_std+precision_porfase_std)\n",
    "\n",
    "    return (acc_patients, acc_patients_std, recall_patients, recall_patients_std, precision_patients, \n",
    "            precision_patients_std, f1_score_patients, f1_score_patients_std, class_recall_patients_mean, \n",
    "            class_recall_patients_std, class_precision_patients_mean, class_precision_patients_std,recall_porfase,\n",
    "            recall_porfase_std, precision_porfase,precision_porfase_std, f1_porfase, f1_porfase_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18e25321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_patients_index(ruta_csv):\n",
    "    data = pd.read_csv(ruta_csv, header=0)\n",
    "    paciente_indices = []\n",
    "    paciente_actual = 0\n",
    "    patients = []\n",
    "    for indice, fila in data.iterrows():\n",
    "        id_paciente = fila[\"id\"].split(\"_\")[0]  # Obtener el número de paciente\n",
    "        if id_paciente != paciente_actual:\n",
    "            patients.append(id_paciente)\n",
    "            inicio = indice\n",
    "            paciente_indices.append(inicio)\n",
    "            paciente_actual = id_paciente\n",
    "    \n",
    "    paciente_indices.append(len(data))\n",
    "    return patients, paciente_indices\n",
    "\n",
    "patients, paciente_indices = obtain_patients_index(\"/home/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29215c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(acc_patients, acc_patients_std, recall_patients, recall_patients_std, precision_patients, \n",
    " precision_patients_std, f1_score_patients, f1_score_patients_std, class_recall_patients_mean, \n",
    " class_recall_patients_std, class_precision_patients_mean, class_precision_patients_std,\n",
    " recall_porfase, recall_porfase_std, precision_porfase, precision_porfase_std, f1_porfase, f1_porfase_std) = metrics_per_patient(all_preds, all_labels, patients, paciente_indices)\n",
    "\n",
    "print('TEST RESULTS: ')\n",
    "print(f'- Accuracy: {acc_patients:.2f} ± {acc_patients_std:.2f}')\n",
    "print(f'- Recall: {recall_patients:.2f} ± {recall_patients_std:.2f}')\n",
    "print(f'- Precision: {precision_patients:.2f} ± {precision_patients_std:.2f}')\n",
    "print(f'- F1_Score: {f1_score_patients:.2f} ± {f1_score_patients_std:.2f}')\n",
    "print(f'- Class recall: {class_recall_patients_mean} ± {class_recall_patients_std}')\n",
    "print(f'- Class precision: {class_precision_patients_mean} ± {class_precision_patients_std}')\n",
    "\n",
    "print('PER PHASE RESULTS: ')\n",
    "print(f'- Recall: {recall_porfase:.2f} ± {recall_porfase_std:.2f}')\n",
    "print(f'- Precision: {precision_porfase:.2f} ± {precision_porfase_std:.2f}')\n",
    "print(f'- F1-Score: {f1_porfase:.2f} ± {f1_porfase_std:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeb10b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain model outputs and labels\n",
    "def obtain_label(model, dataset, dataloader):\n",
    "    numClasses = 7\n",
    "    model.eval()   # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    numSamples = len(dataset)  # Size of the dataset\n",
    "    outputs_m = np.zeros((numSamples, numClasses), dtype=np.float)\n",
    "    labels_m = np.zeros((numSamples,), dtype=np.int)\n",
    "    contSamples = 0\n",
    "\n",
    "    # Iterate over the data\n",
    "    for inputs, labels in dataloader:\n",
    "        batchSize = inputs.size(0)\n",
    "\n",
    "        # Move data to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Apply softmax to the output\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs_m[contSamples:contSamples + batchSize, ...] = outputs.cpu().numpy()\n",
    "            labels_m[contSamples:contSamples + batchSize] = labels.cpu().numpy()\n",
    "            contSamples += batchSize\n",
    "\n",
    "    return outputs_m, labels_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_data = PituDataset(csv_file=\"/home/test_set.csv\",\n",
    "                            root_dir='/home',\n",
    "                            transform=img_transforms)\n",
    "\n",
    "x_test, y_test = prepare_data_features(encoder, test_img_data)\n",
    "x_test = torch.mean(x_test, dim=[2, 3])\n",
    "x_test = scaler.transform(x_test).astype(np.float32)\n",
    "\n",
    "_, test_loader = create_data_loaders_from_arrays(torch.from_numpy(x_train), y_train, torch.from_numpy(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b874c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_test, labels_test = obtain_label(logreg, test_img_data, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "predicted_labels_test = np.argmax(outputs_test, axis=1)\n",
    "cm = confusion_matrix(labels_test, predicted_labels_test, normalize='true')\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Confusion Matrix for SimCLR Model')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fc0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
